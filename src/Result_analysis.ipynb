{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a643b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import os\n",
    "#from pprint import pprint as pp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import LongformerTokenizer, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a705ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43\n",
    "\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b720e62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120\n",
      "614\n",
      "\n",
      "1734\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "first_dataset_doc_path = \"./dataset/First_Phase_Release(Correction)/First_Phase_Text_Dataset/\"\n",
    "second_dataset_doc_path = \"./dataset/Second_Phase_Dataset/Second_Phase_Text_Dataset/\"\n",
    "label_path = [\"./dataset/First_Phase_Release(Correction)/answer.txt\", \"./dataset/Second_Phase_Dataset/answer.txt\"]\n",
    "val_dataset_doc_parh = \"./dataset/validation_dataset/Validation_Release/\"\n",
    "val_label_path = \"./dataset/validation_dataset/answer.txt\"\n",
    "\n",
    "first_dataset_path = [first_dataset_doc_path + file_path for file_path in os.listdir(first_dataset_doc_path)]\n",
    "second_dataset_path = [second_dataset_doc_path + file_path for file_path in os.listdir(second_dataset_doc_path)]\n",
    "train_path = first_dataset_path + second_dataset_path\n",
    "val_path = [val_dataset_doc_parh + file_path for file_path in os.listdir(val_dataset_doc_parh)]\n",
    "\n",
    "#check number of data-path\n",
    "print(len(first_dataset_path)) #1120\n",
    "print(len(second_dataset_path)) #614\n",
    "print()\n",
    "print(len(train_path)) #1734\n",
    "print(len(val_path)) #560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ddfa902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_label_dict(label_path):\n",
    "    label_dict = {}  # y\n",
    "    with open(label_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        file_text = f.read().strip()  \n",
    "\n",
    "    # (id, label, start, end, query) or (id, label, start, end, query, time_org, timefix)\n",
    "    for line in file_text.split(\"\\n\"):\n",
    "        sample = line.split(\"\\t\")  \n",
    "        sample[2], sample[3] = int(sample[2]), int(sample[3])\n",
    "\n",
    "        if sample[0] not in label_dict:\n",
    "            label_dict[sample[0]] = [sample[1:]]\n",
    "        else:\n",
    "            label_dict[sample[0]].append(sample[1:])\n",
    "\n",
    "    return label_dict\n",
    "\n",
    "train_label_dict = create_label_dict(label_path[0])\n",
    "second_dataset_label_dict = create_label_dict(label_path[1])\n",
    "train_label_dict.update(second_dataset_label_dict)\n",
    "val_label_dict = create_label_dict(val_label_path)\n",
    "\n",
    "\n",
    "def load_medical_records(paths):\n",
    "    medical_record_dict = {}\n",
    "    for data_path in paths:\n",
    "\n",
    "        if os.path.isfile(data_path):\n",
    "            file_id = data_path.split(\"/\")[-1].split(\".txt\")[0]\n",
    "            with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                file_text = f.read()\n",
    "                medical_record_dict[file_id] = file_text\n",
    "    return medical_record_dict\n",
    "\n",
    "train_medical_record_dict = load_medical_records(train_path)\n",
    "val_medical_record_dict = load_medical_records(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e443bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_medical_record_dict = {**train_medical_record_dict, **val_medical_record_dict}\n",
    "all_label_dict = {**train_label_dict, **val_label_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6e0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_labels(text, labels, record_id, tag=False):\n",
    "    for i, label in enumerate(labels):  \n",
    "        extracted_text = text[label[1]:label[2]]\n",
    "        if extracted_text != label[3]:\n",
    "            print(f\"Error in ID {record_id}, Line {i}: {label[0]}, position: {label[1]}-{label[2]}, \"\n",
    "                  f\"label: '{label[3]}', extracted: '{extracted_text}'\")\n",
    "        elif tag:\n",
    "            print(f\"Correct in ID {record_id}, Line {i}: {label[0]}, position: {label[1]}-{label[2]}, extracted: '{extracted_text}'\")\n",
    "\n",
    "def check_all_labels(medical_records, label_dict, tag=False):\n",
    "    for record_id, text in medical_records.items():\n",
    "        if record_id in label_dict:\n",
    "            labels = label_dict[record_id]\n",
    "            check_labels(text, labels, record_id, tag)\n",
    "        else:\n",
    "            print(f\"ID: {record_id} has no label\")\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e7c54c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in ID 1139, Line 16: HOSPITAL, position: 2702-2722, label: 'PLANTAGENET HOSPITAL', extracted: 'PLANTAGENE3/9 JENNIE'\n",
      "Error in ID 1481, Line 21: DEPARTMENT, position: 2390-2403, label: 'SEALS Central', extracted: 'SEAKALBARRI H'\n",
      "Error in ID file21297, Line 20: ORGANIZATION, position: 6045-6064, label: 'KB Home Los Angeles', extracted: 'KB Home\tLos Angeles'\n"
     ]
    }
   ],
   "source": [
    "# check training data\n",
    "check_all_labels(all_medical_record_dict, all_label_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec673286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLANTAGENE3/9 JENNIE\n",
      "['HOSPITAL', 2702, 2722, 'PLANTAGENET HOSPITAL']\n"
     ]
    }
   ],
   "source": [
    "# check 1139, PLANTAGENET 3/9 JENNIE COX CLOSE Pathology ?\n",
    "print(all_medical_record_dict['1139'][2702:2722])\n",
    "print(all_label_dict['1139'][16])\n",
    "\n",
    "# replace it\n",
    "all_label_dict['1139'][16][3]=all_medical_record_dict['1139'][2702:2722]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbfb5553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEAKALBARRI H\n",
      "['DEPARTMENT', 2390, 2403, 'SEALS Central']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEPARTMENT', 2390, 2403, 'SEALS Central']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check 1481, there is no DEPARTMENT\n",
    "print(all_medical_record_dict['1481'][2390:2403])\n",
    "print(all_label_dict['1481'][21])\n",
    "\n",
    "# remove it \n",
    "all_label_dict['1481'].pop(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "135403b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check file21297, index 6047 is '\\t'\n",
    "all_medical_record_dict['file21297'][6045:6064]\n",
    "\n",
    "# replace it\n",
    "all_medical_record_dict['file21297'] = val_medical_record_dict['file21297'][:6047] + ' ' + val_medical_record_dict['file21297'][6048:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "792ab4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = list(all_medical_record_dict.keys())\n",
    "random.shuffle(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e92b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Train Set Size: 1835\n",
      "New Validation Set Size: 459\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(all_keys))\n",
    "val_size = len(all_keys) - train_size\n",
    "\n",
    "train_keys = all_keys[:train_size]\n",
    "val_keys = all_keys[train_size:]\n",
    "\n",
    "train_medical_record_dict = {key: all_medical_record_dict[key] for key in train_keys}\n",
    "train_label_dict = {key: all_label_dict[key] for key in train_keys}\n",
    "\n",
    "val_medical_record_dict = {key: all_medical_record_dict[key] for key in val_keys}\n",
    "val_label_dict = {key: all_label_dict[key] for key in val_keys}\n",
    "\n",
    "print(\"New Train Set Size:\", len(train_medical_record_dict))\n",
    "print(\"New Validation Set Size:\", len(val_medical_record_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8d25495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OTHER': 0, 'PATIENT': 1, 'DOCTOR': 2, 'CITY': 3, 'ROOM': 4, 'STREET': 5, 'MEDICALRECORD': 6, 'DEPARTMENT': 7, 'LOCATION-OTHER': 8, 'COUNTRY': 9, 'IDNUM': 10, 'STATE': 11, 'AGE': 12, 'SET': 13, 'HOSPITAL': 14, 'DATE': 15, 'ZIP': 16, 'URL': 17, 'DURATION': 18, 'ORGANIZATION': 19, 'TIME': 20, 'PHONE': 21}\n"
     ]
    }
   ],
   "source": [
    "# fix it\n",
    "labels_type_table={'OTHER': 0, 'PATIENT': 1, 'DOCTOR': 2, 'CITY': 3, 'ROOM': 4, 'STREET': 5, 'MEDICALRECORD': 6, 'DEPARTMENT': 7, 'LOCATION-OTHER': 8, 'COUNTRY': 9, 'IDNUM': 10, 'STATE': 11, 'AGE': 12, 'SET': 13, 'HOSPITAL': 14, 'DATE': 15, 'ZIP': 16, 'URL': 17, 'DURATION': 18, 'ORGANIZATION': 19, 'TIME': 20, 'PHONE': 21}\n",
    "print(labels_type_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f647895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dc4e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerModel\n",
    "from torchcrf import CRF\n",
    "\n",
    "class MyLongformerModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MyLongformerModel, self).__init__()\n",
    "\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(logits, labels, mask=attention_mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(logits, mask=attention_mask.byte())\n",
    "\n",
    "model = MyLongformerModel(num_labels=22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bd5cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf090e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_model_result(model_predict_list, offsets_mapping, labels_type_table):\n",
    "    \"\"\"\n",
    "    Decode the model predictions into a list of labeled segments.\n",
    "\n",
    "    Parameters:\n",
    "    - model_predict_list (list): List of predicted labels from the model.\n",
    "    - offsets_mapping (list): List of offset mappings for the predicted labels.\n",
    "    - labels_type_table (dict): Dictionary mapping label IDs to label names.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of labeled segments, where each segment is represented as (label, start, end).\n",
    "    \"\"\"\n",
    "\n",
    "    id_to_label = {id: label for label, id in labels_type_table.items()}\n",
    "    predict_y = []\n",
    "    pre_label_id = 0\n",
    "\n",
    "    for position_id, label_id in enumerate(model_predict_list):\n",
    "        if label_id != 0:\n",
    "            if pre_label_id != label_id:\n",
    "                start = int(offsets_mapping[position_id][0])\n",
    "            end = int(offsets_mapping[position_id][1])\n",
    "\n",
    "        if pre_label_id != label_id and pre_label_id != 0:\n",
    "            predict_y.append([id_to_label[pre_label_id], start, end])\n",
    "        pre_label_id = label_id\n",
    "\n",
    "    if pre_label_id != 0:\n",
    "        predict_y.append([id_to_label[pre_label_id], start, end])\n",
    "\n",
    "    return predict_y\n",
    "\n",
    "\n",
    "def merge_overlapping_predictions(predictions):\n",
    "    \"\"\"\n",
    "    Merge overlapping labeled segments in a list.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions (list): List of labeled segments, where each segment is represented as (label, start, end).\n",
    "\n",
    "    Returns:\n",
    "    - list: List of merged labeled segments after resolving overlaps.\n",
    "    \"\"\"\n",
    "    if not predictions:\n",
    "        return []\n",
    "\n",
    "    sorted_predictions = sorted(predictions, key=lambda x: x[1])\n",
    "\n",
    "    merged_predictions = [sorted_predictions[0]]\n",
    "    for current in sorted_predictions[1:]:\n",
    "        last = merged_predictions[-1]\n",
    "        if current[0] == last[0] and current[1] <= last[2]:\n",
    "            merged_predictions[-1] = (last[0], last[1], max(last[2], current[2]))\n",
    "        else:\n",
    "            merged_predictions.append(current)\n",
    "\n",
    "    return merged_predictions\n",
    "\n",
    "\n",
    "def predict_text_segments(model, tokenizer, text, max_length, overlap, device):\n",
    "    \"\"\"\n",
    "    Predict labeled segments in a given text using the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - tokenizer: Tokenizer for processing the input text.\n",
    "    - text (str): The input text to be processed.\n",
    "    - max_length (int): Maximum length of text segments for prediction.\n",
    "    - overlap (int): Overlapping length between consecutive text segments.\n",
    "    - device: Device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of predicted labeled segments, where each segment is represented as (label, start, end).\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    offset = 0\n",
    "\n",
    "    for i in range(0, len(text), max_length - overlap):\n",
    "        segment = text[i:i+max_length]\n",
    "        encodings = tokenizer(segment, padding=True, truncation=True, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "        encodings[\"input_ids\"] = encodings[\"input_ids\"].to(device)\n",
    "        encodings[\"attention_mask\"] = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encodings[\"input_ids\"], encodings[\"attention_mask\"])\n",
    "            model_predict_list = outputs[0]  \n",
    "            predictions = decode_model_result(model_predict_list, encodings[\"offset_mapping\"][0], labels_type_table)\n",
    "\n",
    "        adjusted_predictions = [(label, start+offset, end+offset) for label, start, end in predictions]\n",
    "        all_predictions.extend(adjusted_predictions)\n",
    "        offset = i + max_length - overlap\n",
    "\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c73b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(label_name, start, end, text_segment):\n",
    "    \"\"\"\n",
    "    Perform post-processing on labeled segments to refine label information.\n",
    "\n",
    "    Parameters:\n",
    "    - label_name (str): The predicted label for the segment.\n",
    "    - start (int): Start position of the labeled segment.\n",
    "    - end (int): End position of the labeled segment.\n",
    "    - text_segment (str): The actual text content of the segment.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Processed label, refined start position, refined end position, and updated text content.\n",
    "    \"\"\"\n",
    "    processed_label = label_name.strip()\n",
    "\n",
    "    if processed_label.endswith('-') or processed_label.endswith('\"') or processed_label.endswith(\"'\"):\n",
    "        processed_label = processed_label[:-1]\n",
    "        end -= 1\n",
    "\n",
    "    if processed_label == 'DATE' and text_segment.isdigit() and len(text_segment) > 8:\n",
    "        end = start + 8\n",
    "        text_segment = text_segment[:8]\n",
    "\n",
    "    if processed_label == 'STATE':\n",
    "        if text_segment.endswith('TAS'):\n",
    "            text_segment = 'TAS'\n",
    "            start = end - 3\n",
    "        elif (len(text_segment) >= 3):\n",
    "            if text_segment[0].isupper() and text_segment[1].isupper() and text_segment[2].islower():\n",
    "                if len(text_segment) == 3:\n",
    "                    text_segment = text_segment[:2]\n",
    "                    end -= 1\n",
    "                else:\n",
    "                    text_segment = text_segment[1:]\n",
    "                    start += 1\n",
    "\n",
    "    if processed_label == 'CITY':\n",
    "        if any(text_segment.endswith(suffix) for suffix in ['ONT', 'LET', 'NET', 'LAT']):\n",
    "            end -= 1\n",
    "        elif any(text_segment.endswith(suffix) for suffix in ['RAS', 'CHS', 'LES']):\n",
    "            end -= 1\n",
    "\n",
    "    return processed_label, start, end, text_segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d892e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_continuous_time_labels(predictions):\n",
    "    \"\"\"\n",
    "    Merge continuous time labels that are adjacent in predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions (list of tuples): List of predictions with each tuple containing label name, start position,\n",
    "      end position, and predicted text content.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: Merged predictions where continuous time labels are combined into a single prediction.\n",
    "    \"\"\"\n",
    "    merged_predictions = []\n",
    "    prev_label = None\n",
    "\n",
    "    for label_name, start, end, predict_str in predictions:\n",
    "        if label_name == 'TIME' and prev_label and prev_label['label_name'] == 'TIME':\n",
    "            if prev_label['end'] + 1 == start:\n",
    "                prev_label['predict_str'] += ' ' + predict_str\n",
    "                prev_label['end'] = end\n",
    "                continue\n",
    "\n",
    "        if prev_label:\n",
    "            merged_predictions.append((prev_label['label_name'], prev_label['start'], prev_label['end'], prev_label['predict_str']))\n",
    "\n",
    "        prev_label = {'label_name': label_name, 'start': start, 'end': end, 'predict_str': predict_str}\n",
    "\n",
    "    if prev_label:\n",
    "        merged_predictions.append((prev_label['label_name'], prev_label['start'], prev_label['end'], prev_label['predict_str']))\n",
    "\n",
    "    return merged_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24a3eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_single_sample(model, tokenizer, sample_id, val_medical_record_dict, device, max_length=4096, overlap=512):\n",
    "    \"\"\"\n",
    "    Predict labels for a single medical record sample.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The trained model for making predictions.\n",
    "    - tokenizer: The tokenizer used for encoding the input text.\n",
    "    - sample_id (str): Identifier for the medical record sample.\n",
    "    - val_medical_record_dict (dict): Dictionary containing medical record samples with sample_id as keys and text as values.\n",
    "    - device: Device (e.g., 'cuda' or 'cpu') on which the model should run.\n",
    "    - max_length (int): Maximum length for each text segment during prediction.\n",
    "    - overlap (int): Overlap size between consecutive text segments during prediction.\n",
    "\n",
    "    Returns:\n",
    "    - str: String containing the predicted labels in the required format for the given medical record sample.\n",
    "    \"\"\"\n",
    "    output_string = \"\"\n",
    "    sample_text = val_medical_record_dict[sample_id]\n",
    "    predictions = predict_text_segments(model, tokenizer, sample_text, max_length, overlap, device)\n",
    "    final_predictions = merge_overlapping_predictions(predictions)\n",
    "\n",
    "    extended_predictions = [(label_name, start, end, sample_text[start:end]) for label_name, start, end in final_predictions]\n",
    "\n",
    "    merged_predictions = merge_continuous_time_labels(extended_predictions)\n",
    "\n",
    "    for label_name, start, end, predict_str in merged_predictions:\n",
    "        label_name, start, end, predict_str = post_processing(label_name, start, end, predict_str)\n",
    "        sample_result_str = f\"{sample_id}\\t{label_name}\\t{start}\\t{end}\\t{predict_str}\\n\"\n",
    "        output_string += sample_result_str\n",
    "\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d18060",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d641b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_entire_dataset(model, tokenizer, val_medical_record_dict, device, max_length=4096, overlap=512):\n",
    "    \"\"\"\n",
    "    Predict labels for an entire dataset of medical record samples.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The trained model for making predictions.\n",
    "    - tokenizer: The tokenizer used for encoding the input text.\n",
    "    - val_medical_record_dict (dict): Dictionary containing medical record samples with sample_id as keys and text as values.\n",
    "    - device: Device (e.g., 'cuda' or 'cpu') on which the model should run.\n",
    "    - max_length (int): Maximum length for each text segment during prediction.\n",
    "    - overlap (int): Overlap size between consecutive text segments during prediction.\n",
    "\n",
    "    Returns:\n",
    "    - str: String containing the predicted labels for the entire dataset in the required format.\n",
    "    \"\"\"\n",
    "    output_string = \"\"\n",
    "    for sample_id, sample_text in val_medical_record_dict.items():\n",
    "        predictions = predict_text_segments(model, tokenizer, sample_text, max_length, overlap, device)\n",
    "        final_predictions = merge_overlapping_predictions(predictions)\n",
    "\n",
    "        extended_predictions = [(label_name, start, end, sample_text[start:end]) for label_name, start, end in final_predictions]\n",
    "\n",
    "        merged_predictions = merge_continuous_time_labels(extended_predictions)\n",
    "\n",
    "        for label_name, start, end, predict_str in merged_predictions:\n",
    "            label_name, start, end, predict_str = post_processing(label_name, start, end, predict_str)\n",
    "            sample_result_str = f\"{sample_id}\\t{label_name}\\t{start}\\t{end}\\t{predict_str}\\n\"\n",
    "            output_string += sample_result_str\n",
    "\n",
    "    return output_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Predicted with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbadf714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ner(ground_truth, predictions, category):\n",
    "    \"\"\"\n",
    "    Compare predicted Named Entity Recognition (NER) results with ground truth for a specific category.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth (dict): Ground truth labeled data with document IDs as keys and associated labels.\n",
    "    - predictions (dict): Predicted labeled data with document IDs as keys and associated labels.\n",
    "    - category (str): Specific NER category to evaluate.\n",
    "\n",
    "    Prints:\n",
    "    - Outputs differences between ground truth and predictions for the specified category.\n",
    "    - Calculates and prints Precision, Recall, and F1-Score for the specified category.\n",
    "    \"\"\"\n",
    "    def extract_entities(label_dict, category):\n",
    "        \"\"\"\n",
    "        Extract entities of a specific category from a label dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        - label_dict (dict): Dictionary containing labels with document IDs as keys and associated labels.\n",
    "        - category (str): Specific NER category to extract.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Dictionary with entities and their corresponding labels.\n",
    "        \"\"\"\n",
    "        entities = {}\n",
    "        for doc_id, labels in label_dict.items():\n",
    "            for label in labels:\n",
    "                if label[0] == category:\n",
    "                    entities[(doc_id, tuple(label[1:3]))] = label[3]\n",
    "        return entities\n",
    "\n",
    "    gt_entities = extract_entities(ground_truth, category)\n",
    "    pred_entities = extract_entities(predictions, category)\n",
    "\n",
    "    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    TP = len([e for e in pred_entities if e in gt_entities and pred_entities[e] == gt_entities[e]])\n",
    "    FP = len([e for e in pred_entities if e not in gt_entities or pred_entities[e] != gt_entities[e]])\n",
    "    FN = len([e for e in gt_entities if e not in pred_entities])\n",
    "\n",
    "    # Print differences\n",
    "    print(f\"Differences in '{category}':\")\n",
    "    for e in pred_entities:\n",
    "        if e not in gt_entities or pred_entities[e] != gt_entities[e]:\n",
    "            print(f\"Predicted but incorrect or not in ground truth: {e}, Prediction: '{pred_entities[e]}'\")\n",
    "\n",
    "    for e in gt_entities:\n",
    "        if e not in pred_entities:\n",
    "            print(f\"Missing in predictions: {e}, Ground Truth: '{gt_entities[e]}'\")\n",
    "\n",
    "    # Calculate Precision, Recall, F1\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    print(f\"\\nPrecision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate macro scores from predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f23b1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macro_scores(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Calculate Macro Precision, Recall, and F1-Score across different categories for Named Entity Recognition (NER).\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth (dict): Ground truth labeled data with document IDs as keys and associated labels.\n",
    "    - predictions (dict): Predicted labeled data with document IDs as keys and associated labels.\n",
    "\n",
    "    Prints:\n",
    "    - Macro Precision, Recall, and F1-Score.\n",
    "\n",
    "    Note:\n",
    "    - Assumes the labeled data format with (doc_id, (start, end), label, text) for each entity.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    calculate_macro_scores(ground_truth, predictions)\n",
    "    ```\n",
    "\n",
    "    The function calculates the Macro Precision, Recall, and F1-Score across different categories for NER.\n",
    "\n",
    "    \"\"\"\n",
    "    def extract_entities(label_dict):\n",
    "        \"\"\"\n",
    "        Extract entities from a label dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        - label_dict (dict): Dictionary containing labels with document IDs as keys and associated labels.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Dictionary with entities and their corresponding labels.\n",
    "        \"\"\"\n",
    "        entities = {}\n",
    "        for doc_id, labels in label_dict.items():\n",
    "            for label in labels:\n",
    "                key = (doc_id, tuple(label[1:3]), label[0])\n",
    "                entities[key] = label[3]\n",
    "        return entities\n",
    "\n",
    "    gt_entities = extract_entities(ground_truth)\n",
    "    pred_entities = extract_entities(predictions)\n",
    "\n",
    "    # Organize entities by category\n",
    "    categories = set([key[2] for key in gt_entities.keys()] + [key[2] for key in pred_entities.keys()])\n",
    "\n",
    "    total_precision, total_recall, total_f1 = 0, 0, 0\n",
    "    for category in categories:\n",
    "        # Filter entities by category\n",
    "        gt_cat = {k: v for k, v in gt_entities.items() if k[2] == category}\n",
    "        pred_cat = {k: v for k, v in pred_entities.items() if k[2] == category}\n",
    "\n",
    "        # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "        TP = len([e for e in pred_cat if e in gt_cat and pred_cat[e] == gt_cat[e]])\n",
    "        FP = len([e for e in pred_cat if e not in gt_cat or pred_cat[e] != gt_cat[e]])\n",
    "        FN = len([e for e in gt_cat if e not in pred_cat])\n",
    "\n",
    "        # Calculate Precision, Recall, F1 for this category\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1_score\n",
    "\n",
    "    # Calculate Macro Precision, Recall, F1\n",
    "    num_categories = len(categories)\n",
    "    macro_precision = total_precision / num_categories\n",
    "    macro_recall = total_recall / num_categories\n",
    "    macro_f1 = total_f1 / num_categories\n",
    "\n",
    "    print(f\"Macro Precision: {macro_precision:.4f}, Macro Recall: {macro_recall:.4f}, Macro F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "062906cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['longformer-crf_43_11_0.9824877389237668', 'longformer-crf_43_12_0.9778169732490528', 'longformer-crf_43_13_0.9842817215780284', 'longformer-crf_43_14_0.9834499750541934', 'longformer-crf_43_15_0.9790109505011605', 'longformer-crf_43_16_0.9846767433239177', 'longformer-crf_43_17_0.981072594189133', 'longformer-crf_43_18_0.9845563964585489', 'longformer-crf_43_19_0.9840764474056046']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def get_model_files(model_dir, model_name, seed):\n",
    "    \"\"\"\n",
    "    Retrieve a list of model files from a directory based on the model name and seed.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dir (str): Directory path containing the model files.\n",
    "    - model_name (str): Name of the model to search for in the file names.\n",
    "    - seed (int): Seed value used during training.\n",
    "\n",
    "    Returns:\n",
    "    - list: A sorted list of model files matching the specified criteria.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    model_dir = 'model/'\n",
    "    model_name = 'longformer-crf'\n",
    "    seed = 42\n",
    "    model_files = get_model_files(model_dir, model_name, seed)\n",
    "    print(model_files)\n",
    "    ```\n",
    "\n",
    "    The function searches for model files in the specified directory based on the provided model name and seed.\n",
    "    It returns a sorted list of model files, assuming that the epoch information is located in the third part of the file name.\n",
    "\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"{model_name}_{seed}_\\d+_\\d+\\.\\d+\")\n",
    "    model_files = []\n",
    "\n",
    "    for file in os.listdir(model_dir):\n",
    "        if pattern.match(file):\n",
    "            model_files.append(file)\n",
    "\n",
    "    model_files.sort(key=lambda x: int(x.split('_')[2]))  # Assuming epoch is always in the third part of the file name\n",
    "    return model_files\n",
    "\n",
    "model_dir = 'model/'\n",
    "model_name = 'longformer-crf'\n",
    "seed = 42\n",
    "model_files = get_model_files(model_dir, model_name, seed)\n",
    "print(model_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict all data from all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9105abce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\truer\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torchcrf\\__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorCompare.cpp:519.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.9161, Macro Recall: 0.8772, Macro F1-Score: 0.8903\n",
      "None longformer-crf_43_11_0.9824877389237668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_path in model_files:\n",
    "#    model_path = './model/longformer-crf_42_13_0.9825048099176974'\n",
    "    model.load_state_dict(torch.load('./model/' + model_path, map_location=torch.device('cuda')))\n",
    "\n",
    "    output_string = predict_for_entire_dataset(model, tokenizer, val_medical_record_dict, device)\n",
    "\n",
    "    file_text = output_string\n",
    "    predict = {}\n",
    "    for line in file_text.split(\"\\n\"):\n",
    "        if line.strip():  \n",
    "            sample = line.split(\"\\t\")\n",
    "\n",
    "            if len(sample) >= 4:\n",
    "                sample[2], sample[3] = int(sample[2]), int(sample[3])\n",
    "\n",
    "                if sample[0] not in predict:\n",
    "                    predict[sample[0]] = [sample[1:]]\n",
    "                else:\n",
    "                    predict[sample[0]].append(sample[1:])\n",
    "    ans = calculate_macro_scores(val_label_dict, predict)\n",
    "    print(ans, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "511e82de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences in 'CITY':\n",
      "Predicted but incorrect or not in ground truth: ('1836', (89, 92)), Prediction: 'ACT'\n",
      "Predicted but incorrect or not in ground truth: ('1394', (101, 109)), Prediction: 'Victoria'\n",
      "Predicted but incorrect or not in ground truth: ('1489', (88, 90)), Prediction: 'QL'\n",
      "Predicted but incorrect or not in ground truth: ('1412', (77, 88)), Prediction: 'BONDI NORTH'\n",
      "Missing in predictions: ('1836', (77, 89)), Ground Truth: 'THE ENTRANCE'\n",
      "Missing in predictions: ('1394', (92, 101)), Ground Truth: 'SINGLETON'\n",
      "Missing in predictions: ('1489', (79, 88)), Ground Truth: 'THIRLMERE'\n",
      "\n",
      "Precision: 0.9840, Recall: 0.9880, F1-Score: 0.9860\n"
     ]
    }
   ],
   "source": [
    "compare_ner(val_label_dict, predict, 'CITY')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
