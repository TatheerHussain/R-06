{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c453fb7d",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8de1fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import os\n",
    "#from pprint import pprint as pp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import LongformerTokenizer, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208c173",
   "metadata": {},
   "source": [
    "# Load Dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e84758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1121\n",
      "615\n",
      "\n",
      "1736\n",
      "561\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_dataset_doc_path = \"./dataset/First_Phase_Release(Correction)/First_Phase_Text_Dataset/\"\n",
    "second_dataset_doc_path = \"./dataset/Second_Phase_Dataset/Second_Phase_Text_Dataset/\"\n",
    "label_path = [\"./dataset/First_Phase_Release(Correction)/answer.txt\", \"./dataset/Second_Phase_Dataset/answer.txt\"]\n",
    "val_dataset_doc_parh = \"./dataset/validation_dataset/Validation_Release/\"\n",
    "val_label_path = \"./dataset/validation_dataset/answer.txt\"\n",
    "\n",
    "first_dataset_path = [first_dataset_doc_path + file_path for file_path in os.listdir(first_dataset_doc_path)]\n",
    "second_dataset_path = [second_dataset_doc_path + file_path for file_path in os.listdir(second_dataset_doc_path)]\n",
    "train_path = first_dataset_path + second_dataset_path\n",
    "val_path = [val_dataset_doc_parh + file_path for file_path in os.listdir(val_dataset_doc_parh)]\n",
    "\n",
    "#check number of data-path\n",
    "print(len(first_dataset_path)) #1120\n",
    "print(len(second_dataset_path)) #614\n",
    "print()\n",
    "print(len(train_path)) #1734\n",
    "print(len(val_path)) #560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5de73ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_label_dict(label_path):\n",
    "    \"\"\"\n",
    "    Read labeled data from a file and create dictionaries for training and validation datasets.\n",
    "\n",
    "    uses the `create_label_dict` function to read label files, addressing the potential UTF-8 BOM issue (U+FEFF) by using the `utf-8-sig` encoding.\n",
    "\n",
    "    # Function: create_label_dict\n",
    "\n",
    "    ## Description\n",
    "    This function reads a label file and creates a dictionary containing labeled data. It removes the UTF-8 BOM if present using the `utf-8-sig` encoding.\n",
    "\n",
    "    ## Parameters\n",
    "    - `label_path`: The path to the label file that needs to be processed.\n",
    "\n",
    "    ## Returns\n",
    "    - `label_dict`: A dictionary containing labeled data with unique IDs as keys and corresponding label information as values.\n",
    "    \"\"\"\n",
    "    label_dict = {} \n",
    "    with open(label_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        file_text = f.read().strip()  \n",
    "\n",
    "    # (id, label, start, end, query) or (id, label, start, end, query, time_org, timefix)\n",
    "    for line in file_text.split(\"\\n\"):\n",
    "        sample = line.split(\"\\t\")  \n",
    "        sample[2], sample[3] = int(sample[2]), int(sample[3])\n",
    "\n",
    "        if sample[0] not in label_dict:\n",
    "            label_dict[sample[0]] = [sample[1:]]\n",
    "        else:\n",
    "            label_dict[sample[0]].append(sample[1:])\n",
    "\n",
    "    return label_dict\n",
    "\n",
    "train_label_dict = create_label_dict(label_path[0])\n",
    "second_dataset_label_dict = create_label_dict(label_path[1])\n",
    "train_label_dict.update(second_dataset_label_dict)\n",
    "val_label_dict = create_label_dict(val_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f398c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_medical_records(paths):\n",
    "    \"\"\"\n",
    "    Function to load medical records from text files and create dictionaries for training and validation datasets.\n",
    "\n",
    "    Description :\n",
    "    This function takes a list of file paths, reads the corresponding text files, and creates a dictionary containing medical records.\n",
    "    Each file is identified by its unique ID, extracted from the file path.\n",
    "\n",
    "    Parameters :\n",
    "    - `paths`: A list of file paths containing medical records.\n",
    "\n",
    "    Returns :\n",
    "    - `medical_record_dict`: A dictionary containing medical records, where file IDs are used as keys, and the corresponding text content is the value.\n",
    "    \"\"\"\n",
    "    medical_record_dict = {}\n",
    "    for data_path in paths:\n",
    "\n",
    "        if os.path.isfile(data_path):\n",
    "            file_id = data_path.split(\"/\")[-1].split(\".txt\")[0]\n",
    "            with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                file_text = f.read()\n",
    "                medical_record_dict[file_id] = file_text\n",
    "    return medical_record_dict\n",
    "\n",
    "train_medical_record_dict = load_medical_records(train_path)\n",
    "val_medical_record_dict = load_medical_records(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c30475e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1734\n",
      "1734\n",
      "560\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "#chect the number of data\n",
    "print(len(list(train_medical_record_dict.keys()))) #1734\n",
    "print(len(list(train_label_dict.keys()))) #1734\n",
    "print(len(list(val_medical_record_dict.keys()))) #560\n",
    "print(len(list(val_label_dict.keys()))) #560"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e42c9",
   "metadata": {},
   "source": [
    "# Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3486cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_labels(text, labels, record_id, tag=False):\n",
    "    \"\"\"\n",
    "    Check if the extracted labels from the text match the expected labels.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The full text of the medical record.\n",
    "    - labels (list): A list of labels containing (id, start, end, expected_text).\n",
    "    - record_id (str): The identifier of the medical record.\n",
    "    - tag (bool): If True, print correct extractions as well.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for i, label in enumerate(labels):  \n",
    "        extracted_text = text[label[1]:label[2]]\n",
    "        if extracted_text != label[3]:\n",
    "            print(f\"Error in ID {record_id}, Line {i}: {label[0]}, position: {label[1]}-{label[2]}, \"\n",
    "                  f\"label: '{label[3]}', extracted: '{extracted_text}'\")\n",
    "        elif tag:\n",
    "            print(f\"Correct in ID {record_id}, Line {i}: {label[0]}, position: {label[1]}-{label[2]}, extracted: '{extracted_text}'\")\n",
    "\n",
    "def check_all_labels(medical_records, label_dict, tag=False):\n",
    "    \"\"\"\n",
    "    Check labels for all medical records against the provided label dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - medical_records (dict): A dictionary with medical record IDs as keys and corresponding text as values.\n",
    "    - label_dict (dict): A dictionary with medical record IDs as keys and lists of labels as values.\n",
    "    - tag (bool): If True, print correct extractions as well.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for record_id, text in medical_records.items():\n",
    "        if record_id in label_dict:\n",
    "            labels = label_dict[record_id]\n",
    "            check_labels(text, labels, record_id, tag)\n",
    "        else:\n",
    "            print(f\"ID: {record_id} has no label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0783f46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in ID 1139, Line 16: HOSPITAL, position: 2702-2722, label: 'PLANTAGENET HOSPITAL', extracted: 'PLANTAGENE3/9 JENNIE'\n",
      "Error in ID 1481, Line 21: DEPARTMENT, position: 2390-2403, label: 'SEALS Central', extracted: 'SEAKALBARRI H'\n"
     ]
    }
   ],
   "source": [
    "# check training data\n",
    "check_all_labels(train_medical_record_dict, train_label_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e1f9c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLANTAGENE3/9 JENNIE\n",
      "['HOSPITAL', 2702, 2722, 'PLANTAGENET HOSPITAL']\n"
     ]
    }
   ],
   "source": [
    "# check 1139, PLANTAGENET 3/9 JENNIE COX CLOSE Pathology ?\n",
    "print(train_medical_record_dict['1139'][2702:2722])\n",
    "print(train_label_dict['1139'][16])\n",
    "\n",
    "# replace it\n",
    "train_label_dict['1139'][16][3]=train_medical_record_dict['1139'][2702:2722]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c8448a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEAKALBARRI H\n",
      "['DEPARTMENT', 2390, 2403, 'SEALS Central']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEPARTMENT', 2390, 2403, 'SEALS Central']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check 1481, there is no DEPARTMENT\n",
    "print(train_medical_record_dict['1481'][2390:2403])\n",
    "print(train_label_dict['1481'][21])\n",
    "\n",
    "# remove it \n",
    "train_label_dict['1481'].pop(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d77a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in ID file21297, Line 20: ORGANIZATION, position: 6045-6064, label: 'KB Home Los Angeles', extracted: 'KB Home\tLos Angeles'\n"
     ]
    }
   ],
   "source": [
    "# check val data\n",
    "check_all_labels(val_medical_record_dict, val_label_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5c7541b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check file21297, index 6047 is '\\t'\n",
    "val_medical_record_dict['file21297'][6045:6064]\n",
    "\n",
    "# replace it\n",
    "val_medical_record_dict['file21297'] = val_medical_record_dict['file21297'][:6047] + ' ' + val_medical_record_dict['file21297'][6048:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450747a3",
   "metadata": {},
   "source": [
    "# Create label type table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e640c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OTHER': 0, 'DATE': 1, 'ZIP': 2, 'IDNUM': 3, 'MEDICALRECORD': 4, 'CITY': 5, 'STREET': 6, 'DURATION': 7, 'TIME': 8, 'SET': 9, 'URL': 10, 'PATIENT': 11, 'ROOM': 12, 'HOSPITAL': 13, 'COUNTRY': 14, 'AGE': 15, 'LOCATION-OTHER': 16, 'DOCTOR': 17, 'PHONE': 18, 'STATE': 19, 'ORGANIZATION': 20, 'DEPARTMENT': 21}\n"
     ]
    }
   ],
   "source": [
    "#add special token [other] in label list\n",
    "labels_type = list(set( [label[0] for labels in train_label_dict.values() for label in labels] ))\n",
    "labels_type = [\"OTHER\"] + labels_type \n",
    "labels_num = len(labels_type)\n",
    "labels_type_table = {label_name:id for id, label_name in enumerate(labels_type)}\n",
    "print(labels_type_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3025e492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OTHER': 0, 'PATIENT': 1, 'DOCTOR': 2, 'CITY': 3, 'ROOM': 4, 'STREET': 5, 'MEDICALRECORD': 6, 'DEPARTMENT': 7, 'LOCATION-OTHER': 8, 'COUNTRY': 9, 'IDNUM': 10, 'STATE': 11, 'AGE': 12, 'SET': 13, 'HOSPITAL': 14, 'DATE': 15, 'ZIP': 16, 'URL': 17, 'DURATION': 18, 'ORGANIZATION': 19, 'TIME': 20, 'PHONE': 21}\n"
     ]
    }
   ],
   "source": [
    "# fix it\n",
    "labels_type_table={'OTHER': 0, 'PATIENT': 1, 'DOCTOR': 2, 'CITY': 3, 'ROOM': 4, 'STREET': 5, 'MEDICALRECORD': 6, 'DEPARTMENT': 7, 'LOCATION-OTHER': 8, 'COUNTRY': 9, 'IDNUM': 10, 'STATE': 11, 'AGE': 12, 'SET': 13, 'HOSPITAL': 14, 'DATE': 15, 'ZIP': 16, 'URL': 17, 'DURATION': 18, 'ORGANIZATION': 19, 'TIME': 20, 'PHONE': 21}\n",
    "print(labels_type_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff80d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check the label_type is enough for validation\n",
    "val_labels_type = list(set( [label[0] for labels in val_label_dict.values() for label in labels] ))\n",
    "for val_label_type in val_labels_type:\n",
    "    if val_label_type not in labels_type:\n",
    "        print(\"Special label in validation:\", val_label_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13efa79",
   "metadata": {},
   "source": [
    "# Load Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc240dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c676569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerModel\n",
    "from torchcrf import CRF\n",
    "\n",
    "class MyLongformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom PyTorch model using the Longformer architecture with Conditional Random Fields (CRF) for sequence labeling.\n",
    "\n",
    "    Parameters:\n",
    "    - num_labels (int): The number of unique labels/classes for sequence labeling.\n",
    "\n",
    "    Attributes:\n",
    "    - longformer (LongformerModel): Pre-trained Longformer model.\n",
    "    - dropout (nn.Dropout): Dropout layer to prevent overfitting.\n",
    "    - classifier (nn.Linear): Linear layer for classification.\n",
    "    - crf (CRF): Conditional Random Field layer for sequence labeling.\n",
    "\n",
    "    Methods:\n",
    "    - forward(input_ids, attention_mask, labels=None): Forward pass of the model.\n",
    "\n",
    "    Example Usage:\n",
    "    ```python\n",
    "    model = MyLongformerModel(num_labels=22)\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_labels):\n",
    "        \"\"\"\n",
    "        Initializes the MyLongformerModel.\n",
    "\n",
    "        Parameters:\n",
    "        - num_labels (int): The number of unique labels/classes for sequence labeling.\n",
    "        \"\"\"\n",
    "        super(MyLongformerModel, self).__init__()\n",
    "\n",
    "        # Pre-trained Longformer model\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        # Linear layer for classification\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "        # Conditional Random Field layer for sequence labeling\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the MyLongformerModel.\n",
    "\n",
    "        Parameters:\n",
    "        - input_ids (torch.Tensor): Input tensor containing token IDs.\n",
    "        - attention_mask (torch.Tensor): Attention mask tensor indicating which tokens should be attended to.\n",
    "        - labels (torch.Tensor): Ground truth labels for sequence labeling. If None, decoding is performed.\n",
    "\n",
    "        Returns:\n",
    "        - loss (torch.Tensor) if labels are provided, else decoded sequence labels (torch.Tensor).\n",
    "        \"\"\"\n",
    "        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(logits, labels, mask=attention_mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(logits, mask=attention_mask.byte())\n",
    "\n",
    "# Example Usage\n",
    "model = MyLongformerModel(num_labels=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d4d6aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BACH_SIZE = 4\n",
    "#TRAIN_RATIO = 0.9\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12c210",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "665e09ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PrivacyProtectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset class for privacy protection tasks with medical records.\n",
    "\n",
    "    Parameters:\n",
    "    - medical_record_dict (dict): A dictionary containing medical record IDs as keys and text content as values.\n",
    "    - medical_record_labels (dict): A dictionary containing medical record IDs as keys and lists of labels as values.\n",
    "    - tokenizer: Tokenizer for encoding the text.\n",
    "    - labels_type_table (dict): A dictionary mapping label types to numerical IDs.\n",
    "    - mode (str): Mode of the dataset, e.g., 'train', 'test', or 'val'.\n",
    "\n",
    "    Attributes:\n",
    "    - max_length (int): Maximum length for text chunks.\n",
    "    - labels_type_table (dict): A dictionary mapping label types to numerical IDs.\n",
    "    - tokenizer: Tokenizer for encoding the text.\n",
    "    - data (list): A list containing tuples of text chunks, corresponding labels, and record IDs.\n",
    "\n",
    "    Methods:\n",
    "    - split_and_add_data(text, labels, id): Splits text into chunks and adds data to the dataset.\n",
    "    - __getitem__(index): Returns a tuple containing text chunk, labels, and record ID for a given index.\n",
    "    - __len__(): Returns the total number of items in the dataset.\n",
    "    - find_token_ids(label_start, label_end, offset_mapping): Finds token IDs corresponding to label positions after tokenization.\n",
    "    - encode_labels_position(batch_labels, offset_mapping): Encodes the positions of labels in tokenized text.\n",
    "    - create_labels_tensor(batch_shape, batch_labels_position_encoded): Creates a tensor representing labels for the batch.\n",
    "    - collate_fn(batch_items): Collates a batch of items during data loading.\n",
    "\n",
    "    Example Usage:\n",
    "    ```python\n",
    "    dataset = PrivacyProtectionDataset(medical_record_dict, medical_record_labels, tokenizer, labels_type_table, mode='train')\n",
    "    dataloader = DataLoader(dataset, batch_size=32, collate_fn=dataset.collate_fn)\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, medical_record_dict: dict, medical_record_labels: dict, tokenizer, labels_type_table: dict, mode: str):\n",
    "        \"\"\"\n",
    "        Initializes the PrivacyProtectionDataset.\n",
    "\n",
    "        Parameters:\n",
    "        - medical_record_dict (dict): A dictionary containing medical record IDs as keys and text content as values.\n",
    "        - medical_record_labels (dict): A dictionary containing medical record IDs as keys and lists of labels as values.\n",
    "        - tokenizer: Tokenizer for encoding the text.\n",
    "        - labels_type_table (dict): A dictionary mapping label types to numerical IDs.\n",
    "        - mode (str): Mode of the dataset, e.g., 'train', 'test', or 'val'.\n",
    "        \"\"\"\n",
    "        self.max_length = 4096\n",
    "        self.labels_type_table = labels_type_table\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "\n",
    "        for id, text in medical_record_dict.items():\n",
    "            labels = medical_record_labels.get(id, [])\n",
    "            self.split_and_add_data(text, labels, id)\n",
    "\n",
    "    def split_and_add_data(self, text, labels, id):\n",
    "        \"\"\"\n",
    "        Splits the text into chunks and adds data to the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The full text of the medical record.\n",
    "        - labels (list): A list of labels containing (id, start, end, expected_text).\n",
    "        - id (str): The identifier of the medical record.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Split text into chunks of max_length\n",
    "        for i in range(0, len(text), self.max_length):\n",
    "            text_chunk = text[i:i + self.max_length]\n",
    "            # Adjust labels for this chunk\n",
    "            chunk_labels = [label for label in labels if label[1] >= i and label[2] <= i + self.max_length]\n",
    "            chunk_labels = [[label[0], label[1] - i, label[2] - i] for label in chunk_labels]\n",
    "            self.data.append((text_chunk, chunk_labels, id))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple containing text chunk, labels, and record ID for a given index.\n",
    "\n",
    "        Parameters:\n",
    "        - index (int): Index of the item in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing text chunk, labels, and record ID.\n",
    "        \"\"\"\n",
    "        text_chunk, chunk_labels, id = self.data[index]\n",
    "        return text_chunk, chunk_labels, id\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of items in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - int: The total number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def find_token_ids(self, label_start, label_end, offset_mapping):\n",
    "        \"\"\"\n",
    "        Finds token IDs corresponding to label positions after tokenization.\n",
    "\n",
    "        Parameters:\n",
    "        - label_start (int): Start position of the label.\n",
    "        - label_end (int): End position of the label.\n",
    "        - offset_mapping (list): List of token offset mappings.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing the start and end token IDs.\n",
    "        \"\"\"\n",
    "        encode_start = float(\"inf\")  # max\n",
    "        encode_end = 0\n",
    "        for token_id, token_range in enumerate(offset_mapping):\n",
    "            token_start, token_end = token_range\n",
    "\n",
    "            # if token range one side out of label range, still take the token\n",
    "            if token_start == 0 and token_end == 0:  # special token\n",
    "                continue\n",
    "\n",
    "            if label_start < token_end and label_end > token_start:\n",
    "                if token_id < encode_start:\n",
    "                    encode_start = token_id\n",
    "                encode_end = token_id + 1\n",
    "\n",
    "        return encode_start, encode_end\n",
    "\n",
    "    def encode_labels_position(self, batch_labels: list, offset_mapping: list):\n",
    "        \"\"\"\n",
    "        Encodes the positions of labels in tokenized text.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_labels (list): List of labels for a batch.\n",
    "        - offset_mapping (list): List of token offset mappings for the batch.\n",
    "\n",
    "        Returns:\n",
    "        - list: List of encoded label positions for the batch.\n",
    "        \"\"\"\n",
    "        batch_encoding_labels = []\n",
    "        for sample_labels, sample_offsets in zip(batch_labels, offset_mapping):\n",
    "            encoding_labels = []\n",
    "            for label in sample_labels:\n",
    "                encoding_start, encoding_end = self.find_token_ids(label[1], label[2], sample_offsets)\n",
    "                encoding_labels.append([label[0], encoding_start, encoding_end])\n",
    "            batch_encoding_labels.append(encoding_labels)\n",
    "        return batch_encoding_labels\n",
    "\n",
    "    def create_labels_tensor(self, batch_shape: list, batch_labels_position_encoded: list):\n",
    "        \"\"\"\n",
    "        Creates a tensor representing labels for the batch.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_shape (list): Shape of the tensor to be created.\n",
    "        - batch_labels_position_encoded (list): List of encoded label positions for the batch.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tensor representing labels for the batch.\n",
    "        \"\"\"\n",
    "        if batch_shape[-1] > self.max_length:\n",
    "            batch_shape[-1] = self.max_length\n",
    "        labels_tensor = torch.zeros(batch_shape)\n",
    "\n",
    "        for sample_id in range(batch_shape[0]):\n",
    "            for label in batch_labels_position_encoded[sample_id]:\n",
    "                label_id = self.labels_type_table[label[0]]\n",
    "                start = label[1]\n",
    "                end = label[2]\n",
    "\n",
    "                if start >= self.max_length:\n",
    "                    continue\n",
    "                elif end >= self.max_length:\n",
    "                    end = self.max_length\n",
    "\n",
    "                labels_tensor[sample_id][start:end] = label_id\n",
    "\n",
    "        return labels_tensor\n",
    "\n",
    "    def collate_fn(self, batch_items: list):\n",
    "        \"\"\"\n",
    "        Collates a batch of items during data loading.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_items (list): List of items in the batch.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: A tuple containing tokenized encodings, labels tensor, and original labels.\n",
    "        \"\"\"\n",
    "        # the calculation process in dataloader iteration\n",
    "        batch_medical_record = [sample[0] for sample in batch_items]\n",
    "        batch_labels = [sample[1] for sample in batch_items]\n",
    "        batch_id_list = [sample[2] for sample in batch_items]\n",
    "\n",
    "        encodings = self.tokenizer(batch_medical_record, padding=True, truncation=True, return_tensors=\"pt\",\n",
    "                                   return_offsets_mapping=True)  # truncation=True\n",
    "\n",
    "        batch_labels_position_encoded = self.encode_labels_position(batch_labels, encodings[\"offset_mapping\"])\n",
    "        batch_labels_tensor = self.create_labels_tensor(encodings[\"input_ids\"].shape, batch_labels_position_encoded)\n",
    "\n",
    "        return encodings, batch_labels_tensor, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df7c12a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_id_list = list(train_medical_record_dict.keys())\n",
    "train_medical_record = {sample_id: train_medical_record_dict[sample_id] for sample_id in train_id_list}\n",
    "train_labels = {sample_id: train_label_dict[sample_id] for sample_id in train_id_list}\n",
    "\n",
    "val_id_list = list(val_medical_record_dict.keys())\n",
    "val_medical_record = {sample_id: val_medical_record_dict[sample_id] for sample_id in val_id_list}\n",
    "val_labels = {sample_id: val_label_dict[sample_id] for sample_id in val_id_list}\n",
    "\n",
    "train_dataset = PrivacyProtectionDataset(train_medical_record, train_labels, tokenizer, labels_type_table, \"train\")\n",
    "val_dataset = PrivacyProtectionDataset(val_medical_record, val_labels, tokenizer, labels_type_table, \"validation\")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader( train_dataset, batch_size = BACH_SIZE, shuffle = True, collate_fn = train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader( val_dataset, batch_size = BACH_SIZE, shuffle = False, collate_fn = val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5477f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60201847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device) # Put model on device\n",
    "optim = AdamW(model.parameters(), lr = LEARNING_RATE)\n",
    "#if use CRF\n",
    "#loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "805a87d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_model_result(model_predict_list, offsets_mapping, labels_type_table):\n",
    "    \"\"\"\n",
    "    Decode the model predictions and convert them to a list of labeled tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - model_predict_list (list): List of predicted label indices from the model.\n",
    "    - offsets_mapping (list): List of offset mappings for tokens in the input text.\n",
    "    - labels_type_table (dict): A dictionary mapping label types to numerical IDs.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of labeled tokens with their respective label types, start positions, and end positions.\n",
    "    \"\"\"\n",
    "    id_to_label = {id: label for label, id in labels_type_table.items()}\n",
    "    predict_y = []\n",
    "    pre_label_id = 0\n",
    "\n",
    "    for position_id, label_id in enumerate(model_predict_list):\n",
    "        label_id = label_id.item()\n",
    "        if label_id != 0:\n",
    "            if pre_label_id != label_id:\n",
    "                start = int(offsets_mapping[position_id][0])\n",
    "            end = int(offsets_mapping[position_id][1])\n",
    "\n",
    "        if pre_label_id != label_id and pre_label_id != 0:\n",
    "            predict_y.append([id_to_label[pre_label_id], start, end])\n",
    "        pre_label_id = label_id\n",
    "\n",
    "    if pre_label_id != 0:\n",
    "        predict_y.append([id_to_label[pre_label_id], start, end])\n",
    "\n",
    "    return predict_y\n",
    "\n",
    "\n",
    "def calculate_batch_score(batch_labels, model_predict_tables, offset_mappings, labels_type_table):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1-score for a batch of model predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_labels (list): List of ground truth labels for the batch.\n",
    "    - model_predict_tables (list): List of model predictions for the batch.\n",
    "    - offset_mappings (list): List of offset mappings for tokens in the input text.\n",
    "    - labels_type_table (dict): A dictionary mapping label types to numerical IDs.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing Precision, Recall, and F1-score for the batch.\n",
    "    \"\"\"\n",
    "    score_table = {\"TP\": 0, \"FP\": 0, \"TN\": 0}\n",
    "    batch_size = len(model_predict_tables)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        sample_prediction = decode_model_result(model_predict_tables[batch_id], offset_mappings[batch_id],\n",
    "                                                labels_type_table)\n",
    "        sample_ground_truth = batch_labels[batch_id]\n",
    "\n",
    "        # convert ground truth and predictions to sets for comparison\n",
    "        sample_ground_truth = set([tuple(token) for token in sample_ground_truth])\n",
    "        sample_prediction = set([tuple(token) for token in sample_prediction])\n",
    "\n",
    "        # calculate TP, TN, FP\n",
    "        score_table[\"TP\"] += len(sample_ground_truth & sample_prediction)\n",
    "        score_table[\"TN\"] += len(sample_ground_truth - sample_prediction)\n",
    "        score_table[\"FP\"] += len(sample_prediction - sample_ground_truth)\n",
    "\n",
    "    # avoid division by zero\n",
    "    if (score_table[\"TP\"] + score_table[\"FP\"]) == 0 or (score_table[\"TP\"] + score_table[\"TN\"]) == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    # calculate Precision, Recall, F1_score\n",
    "    Precision = score_table[\"TP\"] / (score_table[\"TP\"] + score_table[\"FP\"])\n",
    "    Recall = score_table[\"TP\"] / (score_table[\"TP\"] + score_table[\"TN\"])\n",
    "    if (Precision + Recall) == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    F1_score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    return Precision, Recall, F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6837da62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:519.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 284.494036\n",
      "Epoch 0 - Precision: 0.938316, Recall: 0.956851, F1 Score: 0.946976\n",
      "Epoch 1 - Train Loss: 23.823733\n",
      "Epoch 1 - Precision: 0.965235, Recall: 0.970781, F1 Score: 0.967732\n",
      "Epoch 2 - Train Loss: 13.653785\n",
      "Epoch 2 - Precision: 0.970470, Recall: 0.973744, F1 Score: 0.971834\n",
      "Epoch 3 - Train Loss: 9.811971\n",
      "Epoch 3 - Precision: 0.956156, Recall: 0.976877, F1 Score: 0.966084\n",
      "Epoch 4 - Train Loss: 7.757423\n",
      "Epoch 4 - Precision: 0.969169, Recall: 0.981579, F1 Score: 0.975109\n",
      "Epoch 5 - Train Loss: 5.331824\n",
      "Epoch 5 - Precision: 0.972039, Recall: 0.980621, F1 Score: 0.976120\n",
      "Epoch 6 - Train Loss: 6.886126\n",
      "Epoch 6 - Precision: 0.976697, Recall: 0.980936, F1 Score: 0.978648\n",
      "Epoch 7 - Train Loss: 5.562029\n",
      "Epoch 7 - Precision: 0.976036, Recall: 0.977340, F1 Score: 0.976481\n",
      "Epoch 8 - Train Loss: 4.213714\n",
      "Epoch 8 - Precision: 0.961607, Recall: 0.972258, F1 Score: 0.966739\n",
      "Epoch 9 - Train Loss: 4.449698\n",
      "Epoch 9 - Precision: 0.974960, Recall: 0.980507, F1 Score: 0.977589\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train and validate a deep learning model for privacy protection using a custom dataset.\n",
    "\n",
    "Parameters:\n",
    "- EPOCH (int): Number of training epochs.\n",
    "- model (torch.nn.Module): The deep learning model to be trained.\n",
    "- train_dataloader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "- val_dataloader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "- optim (torch.optim.Optimizer): The optimizer for updating model parameters during training.\n",
    "- device (torch.device): Device on which the model and data reside (e.g., \"cuda\" or \"cpu\").\n",
    "- labels_type_table (dict): A mapping of label names to corresponding numerical identifiers.\n",
    "\"\"\"\n",
    "\n",
    "train_step = 0\n",
    "val_step = 0\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    \"\"\"\n",
    "    Training and validation loop for each epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - epoch (int): Current epoch number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch_x, batch_y, batch_labels in train_dataloader:\n",
    "        \"\"\"\n",
    "        Training iteration over the batches in the training dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_x (dict): Input features for the batch.\n",
    "        - batch_y (torch.Tensor): Ground truth labels for the batch.\n",
    "        - batch_labels (list): True label information for each token in the batch.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "        train_step += 1\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch_x[\"input_ids\"].to(device)\n",
    "        attention_mask = batch_x[\"attention_mask\"].to(device)\n",
    "        labels = batch_y.long().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        loss = model(input_ids, attention_mask, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch} - Train Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    total_val_f1 = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "\n",
    "    for batch_x, batch_y, batch_labels in val_dataloader:\n",
    "        \"\"\"\n",
    "        Validation iteration over the batches in the validation dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_x (dict): Input features for the batch.\n",
    "        - batch_y (torch.Tensor): Ground truth labels for the batch.\n",
    "        - batch_labels (list): True label information for each token in the batch.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "        val_step += 1\n",
    "        input_ids = batch_x[\"input_ids\"].to(device)\n",
    "        attention_mask = batch_x[\"attention_mask\"].to(device)\n",
    "        labels = batch_y.long().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Inference\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            model_predict_tables = [torch.tensor(pred, device=device) for pred in outputs]\n",
    "\n",
    "            # Calculate precision, recall, and F1 Score\n",
    "            P, R, F1 = calculate_batch_score(\n",
    "                batch_labels, model_predict_tables, batch_x[\"offset_mapping\"], labels_type_table\n",
    "            )\n",
    "            total_precision += P\n",
    "            total_recall += R\n",
    "            total_val_f1 += F1\n",
    "\n",
    "    # Calculate average precision, recall, and F1 Score for the epoch\n",
    "    avg_val_f1 = total_val_f1 / len(val_dataloader)\n",
    "    avg_precision = total_precision / len(val_dataloader)\n",
    "    avg_recall = total_recall / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch} - Precision: {avg_precision:.6f}, Recall: {avg_recall:.6f}, F1 Score: {avg_val_f1:.6f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), \"./model/\" + \"longformer-crf\" + \"_\" + str(epoch) + \"_\" + str(avg_val_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b6c3158",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 1: Precision: 0.9706, Recall: 1.0000, F1 Score: 0.9851\n",
      "Batch 2: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 3: Precision: 0.9412, Recall: 0.9600, F1 Score: 0.9505\n",
      "Batch 4: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 5: Precision: 0.9655, Recall: 1.0000, F1 Score: 0.9825\n",
      "Batch 6: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 7: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 8: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 9: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 10: Precision: 0.9643, Recall: 0.9310, F1 Score: 0.9474\n",
      "Batch 11: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 12: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 13: Precision: 0.9565, Recall: 1.0000, F1 Score: 0.9778\n",
      "Batch 14: Precision: 0.8936, Recall: 0.8936, F1 Score: 0.8936\n",
      "Batch 15: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 16: Precision: 0.9273, Recall: 0.9273, F1 Score: 0.9273\n",
      "Batch 17: Precision: 0.9250, Recall: 0.9737, F1 Score: 0.9487\n",
      "Batch 18: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 19: Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Batch 20: Precision: 0.9859, Recall: 0.9859, F1 Score: 0.9859\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "for i, sample in enumerate(val_dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    # Unpack the sample\n",
    "    encodings, y, batch_labels = sample\n",
    "    batch_size = encodings[\"input_ids\"].shape[0]\n",
    "\n",
    "    # Move inputs to the device\n",
    "    encodings[\"input_ids\"] = encodings[\"input_ids\"].to(device)\n",
    "    encodings[\"attention_mask\"] = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(encodings[\"input_ids\"], encodings[\"attention_mask\"])\n",
    "        model_predict_tables = [torch.tensor(pred, device=device) for pred in outputs]\n",
    "\n",
    "    # Calculate precision, recall, and F1 Score for the batch\n",
    "    P, R, F1 = calculate_batch_score(batch_labels, model_predict_tables, encodings[\"offset_mapping\"], labels_type_table)\n",
    "\n",
    "    # Print the evaluation metrics for the batch\n",
    "    print(f\"Batch {i}: Precision: {P:.4f}, Recall: {R:.4f}, F1 Score: {F1:.4f}\")\n",
    "\n",
    "    # Stop after evaluating 20 batches (adjust as needed)\n",
    "    if i == 20:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
